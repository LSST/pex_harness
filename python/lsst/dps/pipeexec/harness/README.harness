Status 9 August 06
 
		----------------------------------------
		Spreading Pipeline MPI Jobs the LSST Way
		----------------------------------------


LSST image pipeline initiation uses the pipeline mpi-harness located in:
$<SVN>/DC1/src/mw/dps/pipeexec/harness/ .

On teragrid, $(SVN>/DC1/src/mw/dps/pipeexec/harness/SetupHarnessPipeline.py 
is run to initiate the pipeline PBS job.

Syntax:
   ./SetupHarnessPipeline.py --help
   usage: SetupHarnessPipeline.py [options]
 
   options:
     -h, --help          show this help message and exit
     --rootDir=dir       directory containing policy files; default '.'
     --pipeline=file     pipeline policy; default 'Pipeline.policy'
     --batch=file        batch policy; default 'Batch.policy'
     --disk=file         file locator policy; default 'DiskRoot.policy'
     --inputQ=dir        fullpathname of stage0 input queue; default './queue'

All policy files related to a single pipeline execution need to be gathered
into a single directory whose location is provided  BOTH 
   1) on command execution line **AND**
   2) in the login shell startup script as:
   	setenv LSST_POLICY_DIR "<wherever>/policy/".

* The batch policy file defines the operational characteristics of the PBS job
  and the pipeline harness including: number of CPUs to reserve, time limit, 
  PBS queue, number of CCD (slices) in a mosaic, etc.

* The disk policy file defines various directory locators which are specific
  to the science pipeline being executed. It might contain locators for
  log files, working directories, SQL command file directories, ram or gpfs
  directories.  

* The pipeline policy file defines the sequentially ordered stages  comprising
  the pipeline's execution.

The science pipeline executable is the interface between the MPI-harness
managing the individual MPI processes  (aka stages) and the science performed 
within each of those stages.   The name of the science pipeline executable is
defined as the CODE (5th) attribute on the STAGE command in Pipeline.policy.

The locator for the stage0 input queue interfaces to the LSST directory 
containing mosaic images. The mpi-harness removes a processing block on stage0 
when both an image exists in the stage0 input queue and the previous image has
finished stage0 processing.


An existing skeleton science pipeline code is available in:
$SVN/DC1/src/mw/dps/pipeexec/harness/harnessStage.py

Syntax:
   python harnessStage.py -help
   usage: harnessStage.py [options]
 
   options:
     -h, --help            show this help message and exit
     --rootDir=dir         directory containing policy files; default '.'
     --pipelinePolicy=file
                           pipeline policy; default 'Pipeline.policy'
     --diskPolicy=file     disk locator policy; default 'DiskRoot.policy'
     --stageNum=number     stage number within pipeline; required
     --sliceNum=number     CCD slice to be processed; required

This skeleton code prints the equivalent of "I was called" to indicate the
mpi process flow.  The policy files required for both ./SetupHarnessPipeline.py
and ./harnessStage.py are included in the directory:
$SVN/DC1/src/mw/dps/pipeexec/harness/policy/ .


                                                                                
                        -----------------
                        Operational model
                        -----------------
                                                                                
SetupHarnessPipeline.py creates a PBS script and then submits it to PBS;
PBS starts up mpi-harness;
Mpi-harness starts up mpi managers for pipeline, stages, and slices;
Slice manager starts up whatever science code was requested on the
        SetupHarnessPipeline.py command line.
                                                                                
        Please note the Pipeline.policy includes a field for the
        executable name--it is not being used presently (it's on the ToDo list).                                                                                
                                                                                
                        ------------
                        Output Files
                        ------------
Files generated during processing using sample Consumer policies
                                                                                
./StartStages.scr       - PBS script to start the pipeline
./nodelist.scr          - list of cpu nodes reserved for MPI processing
./TestQsub.out          - stdout for PBS job
./TestQsub.err          - stderr for PBS job
./harness.log		- if the nlog logging daemon is not running.
                                                                                

